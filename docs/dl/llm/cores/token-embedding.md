
### embedding(向量化表示)

embedding 是将文本、图像等非数值数据转化为**数值向量**的一种技术。
Word2Vec、GloVe 和 BERT 等模型通过神经网络或统计方法将词语映射为 ****高维实数向量, 也就是一个数组**（例如 100 维、300 维）。这些向量的本质是 **语义的数学表示**

embedding的特点:

- 语义表达能力强:

Embedding 可以捕捉词汇和句子的语义信息。
相似的词语和句子在向量空间中也会相近。**余弦相似性**


- 模型输入优化:

大多数机器学习模型需要数值型输入,Embedding 可以将文本转换为合适的输入格式。


- 计算效率提升:

Embedding 可以大幅压缩文本信息,降低计算复杂度。
基于向量运算的模型计算效率更高。

### 向量数据库

用于存储和管理高维向量数据的数据库系统，支持高效的向量检索和相似度搜索。

使用场景:
- 推荐系统: 根据用户的历史行为和偏好，推荐相似的商品或内容。
- 图像检索: 根据输入图像的特征向量，检索数据库中相似的图像。
- 自然语言处理: 通过文本的向量表示，进行语义搜索和问答系统。






### Embedding 是多模态的基石

| 名词   | 概念                                                                     |
| ------ | ------------------------------------------------------------------------ |
| 分词器（Tokenizer）|  将Prompt拆分为离散的Token |
| embedding |     将Token转换为高维向量                 |
| 编码器  |       提取深层语义特征               |
| 解码器  |       逐步预测下一个Token（如使用 Top-k采样 或 束搜索 或 贪心搜索）               |

- 贪心搜索：选择概率最高的Token（速度快但可能陷入局部最优）；
- 束搜索：保留多个候选序列（平衡质量与速度）；
- 采样方法（如Top-k、核采样）：引入随机性提升多样性


映射表的大小取决于词汇表的大小和 embedding 向量的维度


通过V × D 的矩阵，用来把 token id → 高维向量

