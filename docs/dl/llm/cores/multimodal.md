
### 多模态

多模态扩展的实现方法主要有：

跨模态对齐：通过对比学习或自监督方法，将不同模态的数据映射到同一语义空间。例如，CLIP模型通过最大化文本与图像的相似度，最小化文本与无关图像的相似度，实现跨模态对齐  。
模态融合：在Transformer架构中，为不同模态设计专门的编码器，然后通过注意力机制融合多模态信息。例如，多模态模型可以包含文本编码器、图像编码器和音频编码器，通过跨模态注意力机制融合这些信息。
任务适配：针对特定任务，设计专门的解码器或微调策略。例如，医学领域的多模态模型可以结合医学知识图谱，增强对医学影像和文本的理解能力  。
多模态扩展使大语言模型能够在更广泛的应用场景中发挥作用，如医疗领域的医学影像分析、自动驾驶中的环境感知等  。例如，Meta的Llama 2模型支持文本、图像和视频的多模态输入，能够理解并生成跨模态内容。



- 每种模态（图像/音频/视频/文本）都需要专门的“编码器”把原始信号映射到一个或一系列 d 维的向量 Token。
- 这些向量不是“随便一组随机数”，而是通过预训练（自监督、对比、分类、生成等）得到的，能够保留该模态在语义/时序/空间上的关键信息。



对齐与融合是多模态理解的核心

- 对齐（Alignment）：让不同模态的 Embedding 落到同一个可比的“共享空间”里，常用 Contrastive Loss 让相同语义的多模态对更相似。
- 融合（Fusion）：文本的 embedding 与图像的 embedding 拼接, 通过 Cross-Attention、拼接、联合 Transformer 等方式，让信息互相交融，捕捉更丰富的多模态关联。

图像/音频/视频三者差异与融合思路

- 图像：静态二维信息，常用 CNN/ViT → Patch/全局 Vector；
- 音频：时序一维波形 → Mel-Spectrogram → 声学 Encoder → 时序 Token 序列或全局 Vector；
- 视频：既有时序又有空间，或者“逐帧 + 时序建模”，或直接用 3D Conv/Tubelet → 获得时空特征。

融合策略可以根据任务需求决定：是“图像+文本”对齐做检索，还是“视频+音频+文本”做问答、多模态对话，都用同一个思路：先各自嵌入 → 再对齐 / 融合 → 最终下游 Head 推理或生成。



text2image:

text_encoders
vae
diffusion_models
loras

