
### transformer

利用自注意力机制捕捉文本中的长距离依赖关系


注意力机制的核心思想是：在处理一个元素时，模型可以动态地关注输入序列中的其他部分，从而聚焦于与当前任务最相关的信息。

- 查询（Query）：当前处理的元素。
- 键（Key）和值（Value）：输入序列中的所有元素。

通过**点积（Dot Product）**计算**查询与键**, 来评估序列中不同位置的相关性或重要性，得到注意力权重，然后用这些权重对值进行加权求和，生成当前元素的表示


自注意力机制是一种特殊的注意力机制，其中查询、键和值都来自同一个序列。
作用：捕捉序列内部元素之间的依赖关系。

在Transformer模型中，多头注意力机制是其核心组件

Normalization: 对数据进行重新中心化和重新缩放的调整


| 模块名称                                         | 所在位置       | 主要功能描述                                                                                     |
| ------------------------------------------------ | -------------- | ------------------------------------------------------------------------------------------------ |
| Self-Attention（自注意力）                       | 编码器和解码器 | 使模型在处理每个词时，能够关注序列中所有其他位置的词，以捕捉词与词之间的依赖关系。               |
| Encoder-Decoder Attention（编码器-解码器注意力） | 解码器         | 在解码过程中，允许模型根据编码器的输出，动态地关注输入序列的不同部分，从而有效地利用上下文信息。 |
| Add & Normalize（残差连接与层归一化）            | 所有子层之后   | 通过残差连接缓解梯度消失问题，并通过层归一化稳定训练过程，加速模型收敛。                         |
| Feed Forward（前馈神经网络）                     | 编码器和解码器 | 为每个位置的表示引入非线性变换，增强模型的表达能力。                                             |

