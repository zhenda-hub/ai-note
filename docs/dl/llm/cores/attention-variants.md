

### 注意力机制


| 注意力类型                                   | 主要特点                                                                                             | 应用场景                                        |
| -------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------- |
| **加性注意力（Bahdanau）**                       | 使用可学习的权重矩阵和非线性激活函数（如 tanh）计算注意力得分，适用于**查询和键维度不同**的情况。    | 早期的神经机器翻译模型，处理变长输入序列。      |
| 乘性注意力（Luong）                          | 通过点积计算注意力得分，计算效率高，适用于**查询和键维度相同**的情况。                               | 神经机器翻译、文本摘要等任务。                  |
| **缩放点积注意力(scaled dot-product attention)** | 在点积注意力的基础上引入缩放因子，防止梯度消失，提升训练稳定性。  适用于**查询和键维度相同**的情况。 | Transformer 模型中的核心注意力机制。            |
| **自注意力（Self-Attention）**                   | **查询、键、值来自同一序列**，捕捉序列中元素之间的全局依赖关系。                                         | Transformer 编码器和解码器，BERT、GPT 等模型。  |
| **多头注意力（Multi-Head）**                     | 将注意力机制分成多个头，捕捉不同子空间的信息，提高模型的表达能力。                                   | Transformer 架构中的关键组成部分。              |
| 交叉注意力（Cross-Attention）                | 查询来自一个序列，键和值来自另一个序列，实现信息的交互融合。                                         | 编码器-解码器结构，如图像字幕生成、跨模态任务。 |
| 掩码注意力（Masked Attention）               | 对未来的信息进行屏蔽，确保模型只能访问当前或过去的信息。                                             | 自回归模型中的解码器，如 GPT 系列。             |
| 通道注意力（Channel Attention）              | 关注特征图的通道维度，增强重要通道的响应。                                                           | 图像分类、目标检测等计算机视觉任务。            |
| 空间注意力（Spatial Attention）              | 关注特征图的空间维度，突出关键区域的信息。                                                           | 图像分割、目标定位等视觉任务。                  |



在注意力机制中，计算查询（Query, Q）与所有键（Key, K）的点积之所以能得到相似度得分，是因为点积在数学上衡量了两个向量之间的相似性。具体而言，点积值越大，表示两个向量的方向越接近，即它们在向量空间中的相似度越高。

用户输入为 Q, 查询相似的K对应的V. 
计算注意力得分 
