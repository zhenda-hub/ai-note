
### 模型前置知识

#### 神经网络基础

神经网络是一种模仿人类大脑结构的机器学习模型.

通过模拟人脑的学习方式，具有强大的数据拟合和特征学习能力，是深度学习的重要支柱

##### 基本概念

| 概念       | 类比              | 功能                                         | 数学表示                                      | 常见类型/示例 |
| ---------- | ----------------- | -------------------------------------------- | --------------------------------------------- | ------------- |
| **神经元** | 生物神经元        | 信息处理的基本单元                           | output = activation(∑(weight * input) + bias) | -             |
|            | **权重** 连接强度 | 决定输入对输出的影响                         | w                                             | -             |
|            | **偏置** 阈值     | 调整神经元输出                               | b                                             | -             |
|            | **激活函数** 开关 | 引入非线性                                   | **Sigmoid, Tanh, ReLU, LeakyReLU, ELU**等     | -             |
| **层**     | 楼层              | 信息处理的层次                               | 输入层、隐藏层、输出层                        | -             |
|            | 输入层            | 接收外界数据，作为神经网络的输入             |                                               |               |
|            | **隐藏层**        | 对输入数据进行复杂的**非线性变换，提取特征** |                                               |               |
|            | 输出层            | 输出神经网络的预测结果                       |                                               |               |

###### 网络层 layer

对于 隐藏层的数量 和 各个隐藏层的节点数: 常见策略是逐层减少节点数

通常，我们选择2的若干次幂作为层的宽度。
因为内存在硬件中的分配和寻址方式，这么做往往可以在计算上更高效。


| 层         | 名称           | 作用                                                                         | 概念                                                |
| ---------- | -------------- | ---------------------------------------------------------------------------- | --------------------------------------------------- |
| 全连接层   | nn.Linear      | 每个神经元与上一层的所有神经元相连。它用于提取特征和进行非线性变换。         | 它的每一个输入都通过矩阵-向量乘法得到它的每个输出。 |
| 激活层     | nn.ReLU        | 引入非线性变换                                                               |                                                     |
| 批归一化层 | nn.BatchNorm2d | 对每个批次的输入数据进行标准化，减小不同特征的偏差，帮助加速训练并稳定网络。 |                                                     |
| dropout层  | nn.Dropout     | 随机将一部分神经元的输出设为0                                                |                                                     |

| 层         | 名称           | 作用                                                                                                 |
| ---------- | -------------- | ---------------------------------------------------------------------------------------------------- |
| 卷基层     | nn.Conv2d      | 通过卷积核对输入数据进行卷积操作，提取局部特征                                                       |
| 池化层     | nn.MaxPool2d   | 用于下采样操作，可以减少特征图的尺寸，减少计算量，并防止过拟合。最常见的池化操作是最大池化和平均池化 |
| 循环层     | nn.RNN         | 保留历史信息                                                                                         |
| 自注意力层 | nn.Transformer | 捕捉输入数据中长距离依赖关系                                                                         |


### 模型 model

一个模型由1个或多个layer 组成.

#### sequential

```python
from torch import nn

model = nn.Sequential(
    nn.Conv2d(1,20,5),
    nn.ReLU(),
    nn.Conv2d(20,64,5),
    nn.ReLU()
)
```

#### 自定义复杂逻辑

```python
from torch import nn

class FCNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(FCNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out
```



#### 模型参数

```python
# 查看参数
model.state_dict()

for name, param in model.named_parameters():
    print(name, param.size())

```

模型参数初始化

```python
nn.init.zeros_
nn.init.constant_
nn.init.normal_
```
